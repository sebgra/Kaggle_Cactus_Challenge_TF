# -*- coding: utf-8 -*-
"""Cactus_TF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11CyHtbymh6NjjrwnfXr1Z70NiGgTNa7j
"""

from google.colab import drive
drive.mount("/content/drive")

"""# GPU test"""

import tensorflow as tf

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

# test

with tf.device('/device:GPU:1'):
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
    c = tf.matmul(a, b)

"""# Imports"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import os
from PIL import Image
import pathlib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import pathlib
import math
import cv2

with tf.device('/device:GPU:1'):

  #tf.enable_eager_execution() # --> needed if TF version < 2.0

#   %matplotlib inline

  # INPUT_DIR = '/content/drive/My Drive/Cactus'
  # TRAIN_IMG_DIR = INPUT_DIR+'/train/'
  # PRED_IMG_DIR = INPUT_DIR+'/test/'

  AUTOTUNE = tf.data.experimental.AUTOTUNE

  np.random.seed(1000)

"""# Path"""

with tf.device('/device:GPU:1'):

  labels = pd.read_csv('/content/drive/My Drive/Cactus/train.csv')
  sub = pd.read_csv('/content/drive/My Drive/Cactus/sample_submission.csv')
  train_path = '/content/drive/My Drive/Cactus/train/'
  test_path = '/content/drive/My Drive/Cactus/test/'

with tf.device('/device:GPU:1'):

  print('Num train samples:{0}'.format(len(os.listdir(train_path))))
  print('Num test samples:{0}'.format(len(os.listdir(test_path))))

labels.head() # show head of dataset

labels['has_cactus'].value_counts() # to know proportions of cactus

lab = 'Has cactus','Hasn\'t cactus'
colors=['green','brown']

plt.figure(figsize=(7,7))
plt.pie(labels.groupby('has_cactus').size(), labels=lab,
        labeldistance=1.1, autopct='%1.1f%%',
        colors=colors,shadow=True, startangle=140)
plt.show()

"""# Has Cactus"""

fig,ax = plt.subplots(1,5,figsize=(15,3))

for i, idx in enumerate(labels[labels['has_cactus']==1]['id'][-5:]):
  path = os.path.join(train_path,idx)
  ax[i].imshow(cv2.imread(path)) # [...,[2,1,0]]

"""# Has no cactus """

fig,ax = plt.subplots(1,5,figsize=(15,3))

for i, idx in enumerate(labels[labels['has_cactus']==0]['id'][-5:]):
  path = os.path.join(train_path,idx)
  ax[i].imshow(cv2.imread(path)) # [...,[2,1,0]]

"""# Deep Learning Part"""

from sklearn.model_selection import train_test_split
with tf.device('/device:GPU:1'):

# Hyper parameters
  num_epochs = 25
  num_classes = 2
  batch_size = 128
  learning_rate = 0.002

with tf.device('/device:GPU:1'):

  # data splitting
  train, val = train_test_split(labels, stratify=labels.has_cactus, test_size=0.1)


  n_training_items = train['id'].count()
  n_testing_items = val['id'].count()

train.shape, val.shape

train['has_cactus'].value_counts()

val['has_cactus'].value_counts()

"""# Creating Slices"""

def img_path(img_file, img_type=0):
    """ 
    img_file: name of image file
    img_type: 0 if for training, 1 for evaluation dataset.
    """
    if img_type==0:
        return train_path +img_file
    else:
        return test_path +img_file

with tf.device('/device:GPU:1'):

  # Find the filename and corresponding labels of all images in training dataset.
  train_image_paths = [img_path(x) for x in train['id']] # Split train set --> train part
  train_image_labels = [x for x in train['has_cactus']]

  # Find the filename and corresponding labels of all images in testing dataset.
  test_image_paths = [img_path(x) for x in val['id']] # Split train set --> val part
  test_image_labels = [x for x in val['has_cactus']]



  # Find the filenames of all prediction files.
  path = os.listdir(test_path)

  pred_images_paths = [img_path(x, 1) for x in path] # Submission images --> called test here need to be homogeneated
  n_pred_items = len(pred_images_paths)

"""# Check images type"""

im = Image.open(train_image_paths[0])
print(im.format, im.size, im.mode)
imgplot = plt.imshow(im)

"""# Custom Generator

Note : Preprocessing come before Dataset creation
"""

def load_and_preprocess_image(imagefile):

  def prepocess_image(image):
    
    def normalize_tensor(tensor,mean,std):

      mean_tensor = tf.constant(mean, shape=[1,3])
      std_tensor = tf.constant (std, shape=[1,3])
      tensor = tf.math.subtract(tensor, mean_tensor, name= None)
      tensor = tf.math.divide(tensor,std_tensor, name=None)

      

      return tensor

    image = tf.image.decode_jpeg(image,channels=3) # R,G,B mode, .jpeg format
    image = tf.pad(image, paddings=[[31,31],[31,31],[0,0]] , mode='REFLECT', name=None)
    image = tf.pad(image, paddings=[[1,1],[1,1],[0,0]], mode='CONSTANT', name =None)
    image = image / 255 # when imported with Pytorch images are scaled between 0.0 and 1.0
    #image = normalize_tensor(image,[0.5,0.5,0.5],[0.5,0.5,0.5])

      
    return image

  image = tf.io.read_file(imagefile)
  return prepocess_image(image)

def load_and_preprocess_from_path_label(path, label):
    return load_and_preprocess_image(path), label



load_and_preprocess_image('/content/drive/My Drive/Cactus/train/a84ae1dd950ecec162dc72412bd36fd0.jpg')


plt.imshow(load_and_preprocess_image('/content/drive/My Drive/Cactus/train/a84ae1dd950ecec162dc72412bd36fd0.jpg'))

load_and_preprocess_image('/content/drive/My Drive/Cactus/train/a84ae1dd950ecec162dc72412bd36fd0.jpg').shape

"""# Creating Input Pipeline"""

with tf.device('/device:GPU:1'):

  # Tensorflow Dataset containing all image paths and labels
  train_ds = tf.data.Dataset.from_tensor_slices((train_image_paths, train_image_labels))
  test_ds = tf.data.Dataset.from_tensor_slices((test_image_paths, test_image_labels))
  pred_ds = tf.data.Dataset.from_tensor_slices(pred_images_paths)


  # Load the images into dataset from the path in the dataset
  train_ds = train_ds.map(load_and_preprocess_from_path_label)
  test_ds = test_ds.map(load_and_preprocess_from_path_label)
  pred_ds = pred_ds.map(load_and_preprocess_image)

  # for element in train_ds.as_numpy_iterator():
  #   print(element)

print(train_ds)

"""# Model Implementation"""

with tf.device('/device:GPU:1'):

  model = keras.Sequential([
                            
                            keras.layers.Conv2D(filters = 32, kernel_size= 3, padding='valid', activation = tf.nn.leaky_relu, input_shape=(96,96,3)),
                            keras.layers.ZeroPadding2D(padding=(2, 2)),
                            tf.keras.layers.BatchNormalization(momentum = 0.1, epsilon = 1e-05),
                            tf.keras.layers.LeakyReLU(),
                            tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2), padding='same'),
                            #####
                            keras.layers.Conv2D(filters = 64, kernel_size= 3, padding='valid', activation = tf.nn.leaky_relu, input_shape=(49,49,32)),
                            keras.layers.ZeroPadding2D(padding=(2, 2)),
                            tf.keras.layers.BatchNormalization(momentum = 0.1, epsilon = 1e-05),
                            tf.keras.layers.LeakyReLU(),
                            tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2), padding='valid'),
                            #####
                            keras.layers.Conv2D(filters = 128, kernel_size= 3, padding='valid', activation = tf.nn.leaky_relu, input_shape=(25,25,64)),
                            tf.keras.layers.ZeroPadding2D(padding=(2, 2)),
                            tf.keras.layers.BatchNormalization(momentum = 0.1, epsilon = 1e-05),
                            tf.keras.layers.LeakyReLU(),
                            tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2), padding='valid'),
                            #####
                            keras.layers.Conv2D(filters = 256, kernel_size= 3, padding='valid', activation = tf.nn.leaky_relu, input_shape=(13,13,128)),
                            tf.keras.layers.ZeroPadding2D(padding=(2, 2)),
                            tf.keras.layers.BatchNormalization(momentum = 0.1, epsilon = 1e-05),
                            tf.keras.layers.LeakyReLU(),
                            tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2), padding='valid'),
                            #####
                            keras.layers.Conv2D(filters = 512, kernel_size= 3, padding='valid', activation = tf.nn.leaky_relu, input_shape=(7,7,256)),
                            tf.keras.layers.ZeroPadding2D(padding=(2, 2)),
                            tf.keras.layers.BatchNormalization(momentum = 0.1, epsilon = 1e-05),
                            tf.keras.layers.LeakyReLU(),
                            tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2), padding='valid'),
                            #####
                            tf.keras.layers.AveragePooling2D(pool_size=(3,3),strides=None, padding='valid'),
                            tf.keras.layers.Flatten(),
                            tf.keras.layers.Dense(units=2, activation='linear')




                            
                            
                            
                            ])

  model.compile(optimizer=keras.optimizers.Adamax(learning_rate=0.002),loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])

  model.build((None,96,96,3))
model.summary()

"""# Model training"""

with tf.device('/device:GPU:1'):

  # train the model

  # Hyper parameters
  num_epochs = 25
  num_classes = 2
  batch_size = 128


  steps_per_epoch = int(math.ceil(n_training_items/batch_size))
  # Training Dataset
  train_ds1 = (train_ds.cache()
              .apply(
                  tf.data.experimental.shuffle_and_repeat(buffer_size=n_training_items)
              )
              .batch(batch_size)
              .prefetch(buffer_size=AUTOTUNE)
              )

  # Testing dataset
  test_ds1 = (test_ds.cache()
              .apply(tf.data.experimental.shuffle_and_repeat(buffer_size=n_training_items))
              .batch(batch_size)
              .prefetch(buffer_size=AUTOTUNE))

  # Prediction dataset
  pred_ds1 = (pred_ds
              .cache()
              .batch(batch_size)
              .prefetch(buffer_size=AUTOTUNE)
            )

print(train_ds1, pred_ds1)

with tf.device('/device:GPU:1'):


  model.fit(train_ds1, epochs=num_epochs, steps_per_epoch=steps_per_epoch)

"""# Save Model Parameters"""

# Saving model

model.save_weights('/content/drive/MyDrive/Cactus/TF_checkpoint_GPU')

"""# Testing Phase"""

with tf.device('/device:GPU:1'):

  model.evaluate(test_ds1, steps=n_testing_items)

"""# Prediction & Submission"""

with tf.device('/device:GPU:1'):

  path = os.listdir(test_path)

  logits = model.predict(pred_ds1, steps=n_pred_items)
  predictions = np.argmax(logits, axis=-1)
  print(path)
  names = np.array([x for x in path])
  pred_df = pd.DataFrame(
      {
          "id":names,
          "has_cactus":predictions
      })
  #pred_df.head()

